{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UllasAcharya16/ZeroDay-Attack/blob/main/TABA_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o-hlVLigpE5p"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Zero-Day / Attack Detection (KMeans + PCA + Semi-circular Hull Augmentation)\n",
        "# =========================\n",
        "\n",
        "# Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial import ConvexHull\n",
        "from scipy.spatial import distance\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "\n",
        "# ---- File paths (keep your existing filenames if those are what you have) ----\n",
        "UNLABELED_TRAIN_PATH = '/content/reduced_transactions_200.csv'\n",
        "KNOWN_ATTACK_PATH    = '/content/fraud_transaction.csv'          # rename if you have a different file\n",
        "TEST_DATA_PATH       = '/content/fraudulent_transactions.csv'    # rename if you have a different file\n",
        "\n",
        "def point_in_hull(point, hull_points):\n",
        "    try:\n",
        "        hull = ConvexHull(hull_points)\n",
        "        test_hull = ConvexHull(np.vstack([hull_points, point.reshape(1, -1)]))\n",
        "        # If the point is inside, the hull shouldn't gain new vertices\n",
        "        return len(test_hull.vertices) == len(hull.vertices)\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def create_augmented_cluster_points(cluster_points, semi_circle_radius=0.1):\n",
        "    \"\"\"\n",
        "    Create augmented points with semi-circles for a cluster\n",
        "    (Topology-inspired boundary thickening)\n",
        "    \"\"\"\n",
        "    augmented_points = []\n",
        "    for pivot in cluster_points:\n",
        "        other_points = cluster_points[~np.all(cluster_points == pivot, axis=1)]\n",
        "        if len(other_points) > 0:\n",
        "            distances = distance.cdist([pivot], other_points)\n",
        "            nearest_same = other_points[np.argmin(distances)]\n",
        "            vec = nearest_same - pivot\n",
        "            angle = np.arctan2(vec[1], vec[0]) * 180 / np.pi\n",
        "            angle = (angle + 180) % 360\n",
        "            theta = np.linspace(np.deg2rad(angle - 90), np.deg2rad(angle + 90), 10)\n",
        "            semi_circle = np.column_stack((\n",
        "                pivot[0] + semi_circle_radius * np.cos(theta),\n",
        "                pivot[1] + semi_circle_radius * np.sin(theta)\n",
        "            ))\n",
        "            augmented_points.extend(semi_circle)\n",
        "\n",
        "    return np.vstack([cluster_points, np.array(augmented_points)]) if len(augmented_points) > 0 else cluster_points\n",
        "\n",
        "# Step 2: Load and preprocess training data (unlabeled)\n",
        "print(\"=== STEP 1: Loading and clustering unlabeled training data ===\")\n",
        "df = pd.read_csv(UNLABELED_TRAIN_PATH)\n",
        "df_numeric = df.select_dtypes(include=[np.number])\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df_numeric)\n",
        "\n",
        "# Step 3: PCA Reduction\n",
        "pca = PCA(n_components=2)\n",
        "reduced_data = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Step 4: Clustering (unlabeled - we don't know which is attack/normal yet)\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "labels = kmeans.fit_predict(reduced_data)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "print(f\"Created 2 clusters from training data:\")\n",
        "print(f\"Cluster 0: {np.sum(labels == 0)} points\")\n",
        "print(f\"Cluster 1: {np.sum(labels == 1)} points\")\n",
        "print()\n",
        "\n",
        "# Step 5: Load known attack data to identify which cluster is attack\n",
        "print(\"=== STEP 2: Using known attack data to identify ATTACK cluster ===\")\n",
        "known_attack_df = pd.read_csv(KNOWN_ATTACK_PATH)  # Known attack samples\n",
        "known_attack_numeric = known_attack_df.select_dtypes(include=[np.number])\n",
        "scaled_known_attack = scaler.transform(known_attack_numeric)\n",
        "known_attack_reduced = pca.transform(scaled_known_attack)\n",
        "\n",
        "# Assign known attack points to clusters\n",
        "attack_cluster_assignments, _ = pairwise_distances_argmin_min(known_attack_reduced, centroids)\n",
        "\n",
        "# Determine which cluster has more attack points\n",
        "cluster_0_attack_count = np.sum(attack_cluster_assignments == 0)\n",
        "cluster_1_attack_count = np.sum(attack_cluster_assignments == 1)\n",
        "\n",
        "print(f\"Known attack points in Cluster 0: {cluster_0_attack_count}\")\n",
        "print(f\"Known attack points in Cluster 1: {cluster_1_attack_count}\")\n",
        "\n",
        "# Identify attack and normal clusters\n",
        "if cluster_0_attack_count > cluster_1_attack_count:\n",
        "    attack_cluster_id = 0\n",
        "    normal_cluster_id = 1\n",
        "    print(f\"Cluster 0 identified as ATTACK cluster ({cluster_0_attack_count} attack points)\")\n",
        "    print(f\"Cluster 1 identified as NORMAL cluster ({cluster_1_attack_count} attack points)\")\n",
        "else:\n",
        "    attack_cluster_id = 1\n",
        "    normal_cluster_id = 0\n",
        "    print(f\"Cluster 1 identified as ATTACK cluster ({cluster_1_attack_count} attack points)\")\n",
        "    print(f\"Cluster 0 identified as NORMAL cluster ({cluster_0_attack_count} attack points)\")\n",
        "print()\n",
        "\n",
        "# Step 6: Plot training data with identified clusters\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "colors = ['red' if i == attack_cluster_id else 'blue' for i in range(2)]\n",
        "cluster_names = [f'Attack Cluster ({attack_cluster_id})' if i == attack_cluster_id else f'Normal Cluster ({normal_cluster_id})' for i in range(2)]\n",
        "semi_circle_radius = 0.1\n",
        "\n",
        "# Store augmented boundaries for later use\n",
        "cluster_boundaries = {}\n",
        "\n",
        "for cluster_id in range(2):\n",
        "    cluster_points = reduced_data[labels == cluster_id]\n",
        "    color = colors[cluster_id]\n",
        "\n",
        "    # Plot points\n",
        "    for point in cluster_points:\n",
        "        ax.scatter(point[0], point[1], color=color, edgecolor='black', s=100, zorder=5)\n",
        "\n",
        "    # Create augmented points and store boundary\n",
        "    augmented_points = create_augmented_cluster_points(cluster_points, semi_circle_radius)\n",
        "    cluster_boundaries[cluster_id] = augmented_points\n",
        "\n",
        "    # Draw convex hull\n",
        "    if len(augmented_points) >= 3:\n",
        "        hull = ConvexHull(augmented_points)\n",
        "        for simplex in hull.simplices:\n",
        "            ax.plot(augmented_points[simplex, 0], augmented_points[simplex, 1], 'k-', linewidth=2)\n",
        "\n",
        "# Plot known attack points used for identification\n",
        "ax.scatter(known_attack_reduced[:, 0], known_attack_reduced[:, 1],\n",
        "          c='orange', marker='x', s=100, label='Known Attack (for ID)', zorder=6)\n",
        "\n",
        "plt.title(\"Training Data Clusters with Identified Attack/Normal Labels\", fontsize=16)\n",
        "plt.legend([cluster_names[0], cluster_names[1], 'Known Attack (for ID)'], loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Load and process test data\n",
        "print(\"=== STEP 3: Processing test data ===\")\n",
        "test_df = pd.read_csv(TEST_DATA_PATH)  # Your actual test data\n",
        "test_numeric = test_df.select_dtypes(include=[np.number])\n",
        "scaled_test = scaler.transform(test_numeric)\n",
        "test_reduced = pca.transform(scaled_test)\n",
        "\n",
        "# Step 8: Assign test points to clusters (for visualization)\n",
        "closest_clusters, distances = pairwise_distances_argmin_min(test_reduced, centroids)\n",
        "\n",
        "# Step 9: Plot test + training data with identified cluster boundaries\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "train_colors = ['lightcoral' if i == attack_cluster_id else 'lightblue' for i in range(2)]\n",
        "test_color = 'darkgreen'\n",
        "\n",
        "for cluster_id in range(2):\n",
        "    train_pts = reduced_data[labels == cluster_id]\n",
        "    test_pts = test_reduced[closest_clusters == cluster_id]\n",
        "\n",
        "    cluster_type = \"Attack\" if cluster_id == attack_cluster_id else \"Normal\"\n",
        "\n",
        "    # Plot training points\n",
        "    ax.scatter(train_pts[:, 0], train_pts[:, 1], c=train_colors[cluster_id],\n",
        "              alpha=0.6, label=f'Training {cluster_type} Cluster {cluster_id}')\n",
        "\n",
        "    # Plot test points\n",
        "    ax.scatter(test_pts[:, 0], test_pts[:, 1], c=test_color, edgecolors='black',\n",
        "              s=50, label='Test Data' if cluster_id == 0 else \"\")\n",
        "\n",
        "    # Draw boundaries ONLY around training data\n",
        "    augmented_points = cluster_boundaries[cluster_id]\n",
        "    if len(augmented_points) >= 3:\n",
        "        hull = ConvexHull(augmented_points)\n",
        "        for simplex in hull.simplices:\n",
        "            ax.plot(augmented_points[simplex, 0], augmented_points[simplex, 1], 'k-', linewidth=2)\n",
        "\n",
        "# Plot centroids\n",
        "ax.scatter(centroids[:, 0], centroids[:, 1], c='black', s=200, marker='X',\n",
        "          label='Centroids')\n",
        "\n",
        "plt.title(\"Test Data with Identified Attack/Normal Cluster Boundaries\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 10: SIMPLIFIED Boundary-based classification\n",
        "print(\"=== SIMPLIFIED ZERO-DAY / ATTACK DETECTION ===\")\n",
        "print()\n",
        "\n",
        "def calculate_anomaly_score(d_normal, d_attack):\n",
        "    \"\"\"\n",
        "    Calculate anomaly score for points outside both clusters.\n",
        "    Higher score = more anomalous\n",
        "    \"\"\"\n",
        "    min_distance = min(d_normal, d_attack)\n",
        "    max_distance = max(d_normal, d_attack)\n",
        "\n",
        "    # Anomaly score: combination of minimum distance (isolation) and distance ratio\n",
        "    isolation_score = min_distance  # How far from nearest cluster\n",
        "    balance_score = min_distance / max_distance  # How balanced between clusters\n",
        "\n",
        "    # Combined anomaly score (higher = more anomalous)\n",
        "    anomaly_score = isolation_score * (1 + balance_score)\n",
        "    return anomaly_score\n",
        "\n",
        "for i, point in enumerate(test_reduced):\n",
        "    # Check if point is inside cluster boundaries\n",
        "    inside_normal = point_in_hull(point, cluster_boundaries[normal_cluster_id])\n",
        "    inside_attack = point_in_hull(point, cluster_boundaries[attack_cluster_id])\n",
        "\n",
        "    # SIMPLIFIED LOGIC\n",
        "    if inside_attack:\n",
        "        print(f\"Test Point {i+1}: 🚨 ATTACK\")\n",
        "    elif inside_normal:\n",
        "        print(f\"Test Point {i+1}: ✅ NORMAL\")\n",
        "    else:\n",
        "        # Point is outside both boundaries - NOW we calculate distances and anomaly score\n",
        "        d_norm = np.linalg.norm(point - centroids[normal_cluster_id])\n",
        "        d_attk = np.linalg.norm(point - centroids[attack_cluster_id])\n",
        "        anomaly_score = calculate_anomaly_score(d_norm, d_attk)\n",
        "\n",
        "        if d_attk < d_norm:\n",
        "            print(f\"Test Point {i+1}: ⚠️ ZERO-DAY ATTACK\")\n",
        "            print(f\"   Distance to attack center: {d_attk:.3f}\")\n",
        "            print(f\"   Distance to normal center: {d_norm:.3f}\")\n",
        "            print(f\"   🔍 ANOMALY SCORE: {anomaly_score:.3f}\")\n",
        "        else:\n",
        "            print(f\"Test Point {i+1}: ❓ ANOMALY\")\n",
        "            print(f\"   Distance to normal center: {d_norm:.3f}\")\n",
        "            print(f\"   Distance to attack center: {d_attk:.3f}\")\n",
        "            print(f\"   🔍 ANOMALY SCORE: {anomaly_score:.3f}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"=== SUMMARY ===\")\n",
        "print(\"✅ NORMAL: Point inside normal cluster\")\n",
        "print(\"🚨 ATTACK: Point inside attack cluster\")\n",
        "print(\"⚠️ ZERO-DAY ATTACK: Outside both clusters, closer to attack\")\n",
        "print(\"❓ ANOMALY: Outside both clusters, closer to normal\")\n",
        "print()\n",
        "print(f\"Identified Clusters:\")\n",
        "print(f\"- Attack Cluster: {attack_cluster_id}\")\n",
        "print(f\"- Normal Cluster: {normal_cluster_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZXIesMDbIx8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}